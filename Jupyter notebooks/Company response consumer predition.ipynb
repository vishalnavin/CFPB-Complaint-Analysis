{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:56:24.383810Z",
     "start_time": "2024-04-27T22:56:23.727697Z"
    }
   },
   "source": [
    "from nltk import SnowballStemmer\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Initialize SnowballStemmer with English language\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# Import necessary libraries for machine learning and text processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader as api\n",
    "import spacy\n",
    "\n",
    "def stemming_tokenizer(str_input):\n",
    "    \"\"\"\n",
    "    This function tokenizes the input string and applies stemming to each token.\n",
    "\n",
    "    Parameters:\n",
    "    str_input (str): The input string to be tokenized and stemmed.\n",
    "\n",
    "    Returns:\n",
    "    list: The list of stemmed tokens.\n",
    "    \"\"\"\n",
    "    words = re.sub(r\"[^A-Za-z]\", \" \", str_input).lower().split()\n",
    "    words = [snow_stemmer.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "def TAB_dfm(text, ngrams_range = (1,2), stop_words = 'english', min_prop = .01, max_features=None):\n",
    "    \"\"\"\n",
    "    This function applies CountVectorizer to the input text and returns a DataFrame and a matrix representation of the text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be vectorized.\n",
    "    ngrams_range (tuple): The range of n-values for different n-grams to be extracted.\n",
    "    stop_words (str): 'english' if English stop words are to be removed, else False.\n",
    "    min_prop (float): The minimum proportion of documents a word must be present in for it to be kept.\n",
    "    max_features (int): The maximum number of features to be kept, based on term frequency.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame representation of the text.\n",
    "    numpy.matrix: Matrix representation of the text.\n",
    "    \"\"\"\n",
    "    if stop_words == 'english':\n",
    "        vec = CountVectorizer(\n",
    "            tokenizer = stemming_tokenizer,\n",
    "            stop_words = stop_words,\n",
    "            ngram_range=ngrams_range,\n",
    "            min_df=min_prop,\n",
    "            max_features=max_features,\n",
    "            token_pattern='(?u)\\\\b\\\\w+\\\\b'\n",
    "            )\n",
    "    else:\n",
    "        vec = CountVectorizer(\n",
    "            tokenizer = stemming_tokenizer,\n",
    "            ngram_range=ngrams_range,\n",
    "            min_df=min_prop,\n",
    "            max_features=max_features,\n",
    "            token_pattern='(?u)\\\\b\\\\w+\\\\b'\n",
    "        )\n",
    "\n",
    "    mtx = vec.fit_transform(text).todense()\n",
    "    df = round(pd.DataFrame(mtx, columns=vec.get_feature_names_out()),2)\n",
    "    return df, mtx\n",
    "\n",
    "def kendall_acc(x,y,percentage = True):\n",
    "    \"\"\"\n",
    "    This function calculates the Kendall's tau-a correlation coefficient between two lists.\n",
    "\n",
    "    Parameters:\n",
    "    x, y (list): The two lists for which to calculate the correlation coefficient.\n",
    "    percentage (bool): If True, the result is returned as a percentage.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the correlation coefficient, lower and upper confidence intervals.\n",
    "    \"\"\"\n",
    "    tau, p_value = stats.kendalltau(x, y)\n",
    "    tau_acc = .5+tau/2\n",
    "    tau_se = np.sqrt((tau_acc*(1 - tau_acc))/len(x))\n",
    "    report = pd.DataFrame([tau_acc, tau_acc - 1.96 * tau_se, tau_acc + 1.96 * tau_se],\n",
    "                          index = ['acc', 'lower', 'upper']).T\n",
    "    report = round(report,4)\n",
    "\n",
    "    if percentage is True:\n",
    "        report = report * 100\n",
    "\n",
    "    return report\n",
    "\n",
    "def jaccard_sim(str1, str2):\n",
    "    \"\"\"\n",
    "    This function calculates the Jaccard similarity between two strings.\n",
    "\n",
    "    Parameters:\n",
    "    str1, str2 (str): The two strings for which to calculate the Jaccard similarity.\n",
    "\n",
    "    Returns:\n",
    "    float: The Jaccard similarity between the two strings.\n",
    "    \"\"\"\n",
    "    a = set(stemming_tokenizer(str1))\n",
    "    b = set(stemming_tokenizer(str2))\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def euclidian_dist(docs, y = 0):\n",
    "    \"\"\"\n",
    "    This function calculates the Euclidean distance between the vectors of a list of documents and a specific document.\n",
    "\n",
    "    Parameters:\n",
    "    docs (list): The list of documents.\n",
    "    y (int): The index of the specific document.\n",
    "\n",
    "    Returns:\n",
    "    list: The list of Euclidean distances.\n",
    "    \"\"\"\n",
    "    _, features = np.asarray(TAB_dfm(docs))\n",
    "    distances = [round(float(euclidean_distances([features[y]], [f])),2) for f in features]\n",
    "    return distances\n",
    "\n",
    "def cosine_sim(docs, y = 0):\n",
    "    \"\"\"\n",
    "    This function calculates the cosine similarity between the vectors of a list of documents and a specific document.\n",
    "\n",
    "    Parameters:\n",
    "    docs (list): The list of documents.\n",
    "    y (int): The index of the specific document.\n",
    "\n",
    "    Returns:\n",
    "    list: The list of cosine similarities.\n",
    "    \"\"\"\n",
    "    _, features = np.asarray(TAB_dfm(docs, stop_words = False))\n",
    "    distances = [round(float(cosine_similarity([features[y]], [f])),2) for f in features]\n",
    "    return distances\n",
    "\n",
    "def spacy_parse(text):\n",
    "    \"\"\"\n",
    "    This function parses a text using the Spacy library and returns a DataFrame containing the parsed information.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to be parsed.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the parsed information.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    rows = [[t.text, t.lemma_, t.pos_, t.tag_, t.dep_, spacy.explain(t.pos_), t.is_stop] for t in doc]\n",
    "    cols = (\"text\", \"lemma\", \"POS\", \"Tag\",\"Dep\",\"explain\", \"stopword\")\n",
    "    df = pd.DataFrame(rows, columns=cols)\n",
    "    return df\n",
    "\n",
    "def lemmas_parse(text):\n",
    "    \"\"\"\n",
    "    This function parses a text and returns a string of lemmas that are not pronouns, numbers, symbols, stopwords, spaces or punctuations.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to be parsed.\n",
    "\n",
    "    Returns:\n",
    "    str: A string of lemmas.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    lemmas = [t.lemma_ for t in doc\n",
    "              if t.pos_ not in ('SPACE', 'PRON', 'PUNCT', 'NUM', 'SYM')\n",
    "              if t.is_stop == False]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "def lemmas_dfm(texts):\n",
    "    \"\"\"\n",
    "    This function applies the lemmas_parse function to a list of texts and returns a DataFrame of the results.\n",
    "\n",
    "    Parameters:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame of the results.\n",
    "    \"\"\"\n",
    "    dfms_joined = pd.DataFrame()\n",
    "    for text in texts:\n",
    "        text = [lemmas_parse(text)]\n",
    "        if len(text[0]) > 1:\n",
    "            dfm, _ = TAB_dfm(text, ngrams_range=(0,1), stop_words = False)\n",
    "            dfms_joined = dfms_joined.append(dfm)\n",
    "    return dfms_joined\n",
    "\n",
    "def ner_parse(text):\n",
    "    \"\"\"\n",
    "    This function parses a text using the Spacy library and returns a DataFrame containing the named entities.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to be parsed.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the named entities.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    rows = [[ent.text, ent.start_char, ent.end_char, ent.label_] for ent in doc.ents]\n",
    "    cols = (\"Text\", \"Start\", \"End\", \"Label\")\n",
    "    df = pd.DataFrame(rows, columns=cols)\n",
    "    return df\n",
    "\n",
    "def ner_filter_parse(text):\n",
    "    \"\"\"\n",
    "    This function parses a text using the Spacy library and returns a string of unique named entities that are geographical places.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to be parsed.\n",
    "\n",
    "    Returns:\n",
    "    str: A string of unique named entities that are geographical places.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    ents = [ent.text for ent in doc.ents\n",
    "            if ent.label_ == 'GPE']\n",
    "    return ' '.join(list(set(ents)))\n",
    "\n",
    "def ner_dfm(texts):\n",
    "    \"\"\"\n",
    "    This function applies the ner_filter_parse function to a list of texts and returns a DataFrame of the results.\n",
    "\n",
    "    Parameters:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame of the results.\n",
    "    \"\"\"\n",
    "    dfms_joined = pd.DataFrame()\n",
    "    for text in texts:\n",
    "        text = [ner_filter_parse(text)]\n",
    "        if len(text[0]) > 1:\n",
    "            dfm, _ = TAB_dfm(text, ngrams_range=(0,1), stop_words = False)\n",
    "            dfms_joined = dfms_joined.append(dfm)\n",
    "    return dfms_joined\n",
    "\n",
    "def tokenizer(str_input):\n",
    "    \"\"\"\n",
    "    This function tokenizes the input string.\n",
    "\n",
    "    Parameters:\n",
    "    str_input (str): The input string to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "    list: The list of tokens.\n",
    "    \"\"\"\n",
    "    words = re.sub(r\"[^A-Za-z]\", \" \", str_input).lower().split()\n",
    "    return words\n",
    "\n",
    "def dfm_lookup(text, dict_as_list, ngrams_range = (1,1), min_prop = .01, max_features=None):\n",
    "    \"\"\"\n",
    "    This function applies CountVectorizer to the input text and returns a DataFrame and a matrix representation of the text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be vectorized.\n",
    "    dict_as_list (list): The list of words to be used as the dictionary.\n",
    "    ngrams_range (tuple): The range of n-values for different n-grams to be extracted.\n",
    "    min_prop (float): The minimum proportion of documents a word must be present in for it to be kept.\n",
    "    max_features (int): The maximum number of features to be kept, based on term frequency.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame representation of the text.\n",
    "    numpy.matrix: Matrix representation of the text.\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer(\n",
    "        tokenizer = tokenizer,\n",
    "        stop_words = 'english',\n",
    "        ngram_range=ngrams_range,\n",
    "        min_df=min_prop,\n",
    "        max_features=max_features,\n",
    "        token_pattern='(?u)\\\\b\\\\w+\\\\b'\n",
    "        )\n",
    "\n",
    "    mtx = vec.fit_transform(text).todense()\n",
    "    df = round(pd.DataFrame(mtx, columns=vec.get_feature_names_out()),2)\n",
    "    df = df[df.columns.intersection(dict_as_list)]\n",
    "    row_sums = df.sum(axis=1)\n",
    "    return row_sums"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "4d14699f3f360ee6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:56:25.486297Z",
     "start_time": "2024-04-27T22:56:24.384470Z"
    }
   },
   "source": [
    "# vecSmall.csv contains pre-trained word vectors for a small vocabulary.\n",
    "vecSmall = pd.read_csv('vecSmall.csv', index_col= 0)\n",
    "\n",
    "# wfFile.csv contains word frequency information for a specific corpus.\n",
    "wfFile = pd.read_csv('wfFile.csv', index_col= 0)\n",
    "\n",
    "# filtered_dataset.csv contains the main dataset for analysis. The 'low_memory' parameter is set to False to silence dtypes warning.\n",
    "data = pd.read_csv('filtered_dataset.csv', index_col= 0, low_memory=False)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "2bdcf067c2f5dc55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:56:25.496777Z",
     "start_time": "2024-04-27T22:56:25.487740Z"
    }
   },
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Instantiate the ShuffleSplit class with 1 split, a test size of 40%, and a random state of 42 for reproducibility\n",
    "# This will be used to create a random split of the data into training and testing sets\n",
    "sss = ShuffleSplit(n_splits=1, test_size=0.4, random_state = 42)\n",
    "\n",
    "# Get the number of splitting iterations in the cross-validator\n",
    "# This is not necessary for the split but can be used to check the number of splits\n",
    "sss.get_n_splits(data)\n",
    "\n",
    "# Generate indices to split data into training and test set\n",
    "# next() is used to get the next item from the iterator\n",
    "train_index, test_index = next(sss.split(data))\n",
    "\n",
    "# Use the generated indices to create the training set\n",
    "# iloc is used for indexing via integers\n",
    "data_train = data.iloc[train_index]\n",
    "\n",
    "# Use the generated indices to create the test set\n",
    "data_test = data.iloc[test_index]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "fc38de48e15f3d24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:56:25.501842Z",
     "start_time": "2024-04-27T22:56:25.497393Z"
    }
   },
   "source": [
    "# Import the lowercase function from the pyodbc library\n",
    "from pyodbc import lowercase\n",
    "\n",
    "# The following code is a Python equivalent to the vecCheck function in the vectorFunctions.R script\n",
    "\n",
    "# Define a pipeline for projecting data into embedding space\n",
    "# The pipeline consists of two steps:\n",
    "# 1. TfidfVectorizer: This is used to convert the text data into a matrix of TF-IDF features.\n",
    "#    The vocabulary is set to the index of the wfFile DataFrame and the lowercase parameter is set to False to keep uppercase characters.\n",
    "# 2. TruncatedSVD: This is used for dimensionality reduction. It transforms the data to have the same number of dimensions as the pre-trained model.\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(vocabulary=wfFile.index, lowercase=False)),  \n",
    "    ('lsa', TruncatedSVD(n_components=vecSmall.shape[1])),  \n",
    "])"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "7dc049ca77fd59c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:56:39.697348Z",
     "start_time": "2024-04-27T22:56:25.502485Z"
    }
   },
   "source": [
    "# Fit the pipeline to the 'Consumer complaint narrative' column of the data DataFrame\n",
    "# This step involves transforming the text data into a matrix of TF-IDF features and then reducing the dimensionality of the data\n",
    "pipeline.fit(data['Consumer complaint narrative'])\n",
    "\n",
    "# Transform the 'Consumer complaint narrative' column of the data DataFrame using the fitted pipeline\n",
    "# This step involves projecting the data into the embedding space\n",
    "vdat = pipeline.transform(data['Consumer complaint narrative'])\n",
    "\n",
    "# Convert the embedded data into a DataFrame\n",
    "# The column names are generated dynamically based on the number of dimensions in the embedded data\n",
    "# Each column represents a dimension in the embedding space\n",
    "vdat = pd.DataFrame(vdat, columns=[f'vec{i+1}' for i in range(vdat.shape[1])])\n",
    "\n",
    "# Print the first 10 rows of the DataFrame\n",
    "# This is used to check the transformed data\n",
    "print(vdat.head(10))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       vec1      vec2      vec3      vec4      vec5      vec6      vec7  \\\n",
      "0  0.212466 -0.042737  0.084530  0.234582 -0.099773 -0.061637  0.058663   \n",
      "1  0.329551 -0.031669 -0.051305 -0.084200  0.057401  0.038061  0.136885   \n",
      "2  0.416767 -0.063856  0.078748  0.057971 -0.059908 -0.073256 -0.056827   \n",
      "3  0.347848 -0.108581 -0.083247 -0.047303  0.275795 -0.014486  0.002262   \n",
      "4  0.254178 -0.043507  0.020654  0.090618 -0.097636 -0.013794 -0.006343   \n",
      "5  0.495647  0.115488  0.200288 -0.012283  0.032888  0.195938 -0.017173   \n",
      "6  0.453418 -0.159387 -0.047675 -0.144897 -0.007143  0.039272 -0.045600   \n",
      "7  0.506661 -0.204375 -0.125888 -0.139932 -0.015730 -0.080862 -0.099955   \n",
      "8  0.476338  0.179721  0.241587  0.031774 -0.016536  0.017355  0.087907   \n",
      "9  0.533134  0.111343  0.187160  0.031704  0.013873  0.188898  0.090014   \n",
      "\n",
      "       vec8      vec9     vec10  ...    vec291    vec292    vec293    vec294  \\\n",
      "0  0.002035 -0.061615 -0.012356  ...  0.023678 -0.022664  0.023196  0.015490   \n",
      "1  0.167399  0.085140 -0.040155  ...  0.004887  0.001357 -0.047930 -0.018181   \n",
      "2  0.002724  0.164222  0.016892  ...  0.030218  0.054838  0.025151 -0.000195   \n",
      "3  0.001086 -0.116583  0.019576  ...  0.013997 -0.014818  0.004612 -0.001743   \n",
      "4  0.042713 -0.015163 -0.045401  ...  0.022082 -0.029517  0.014473 -0.003305   \n",
      "5 -0.050924  0.083704 -0.052130  ... -0.013583 -0.043752  0.002494 -0.007929   \n",
      "6 -0.052550 -0.083591  0.039818  ... -0.007969 -0.002762 -0.005052 -0.000528   \n",
      "7 -0.015847 -0.129722  0.012045  ...  0.006799  0.025023  0.049577  0.016647   \n",
      "8 -0.105103 -0.013788 -0.098615  ... -0.002178  0.019692  0.010904  0.001291   \n",
      "9 -0.025468 -0.035508 -0.052599  ...  0.003945  0.006475 -0.016257 -0.022851   \n",
      "\n",
      "     vec295    vec296    vec297    vec298    vec299    vec300  \n",
      "0  0.005477  0.009341  0.022774  0.017434  0.000098 -0.036202  \n",
      "1  0.019025 -0.018446 -0.015602 -0.002173 -0.015745  0.004060  \n",
      "2  0.010810  0.009157 -0.001221 -0.051728 -0.001580 -0.012022  \n",
      "3  0.017743  0.005413  0.008091  0.007267 -0.016822  0.027371  \n",
      "4 -0.000267  0.008892  0.001569  0.014592 -0.019717  0.002070  \n",
      "5  0.014829  0.039124 -0.010337 -0.024404  0.016088  0.000375  \n",
      "6  0.010635  0.003299  0.008749 -0.023206  0.043916  0.010081  \n",
      "7  0.010650  0.011102  0.042726 -0.016462 -0.040568 -0.015233  \n",
      "8 -0.002463  0.001090 -0.006774 -0.006565 -0.023223  0.026247  \n",
      "9  0.034185  0.003569 -0.014585  0.005199  0.001656 -0.016895  \n",
      "\n",
      "[10 rows x 300 columns]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "a9806b0bc9160231",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:56:39.723228Z",
     "start_time": "2024-04-27T22:56:39.698006Z"
    }
   },
   "source": [
    "# Select the training data from the transformed DataFrame 'vdat' using the training indices\n",
    "vdat_train = vdat.iloc[train_index]\n",
    "\n",
    "# Select the testing data from the transformed DataFrame 'vdat' using the testing indices\n",
    "vdat_test = vdat.iloc[test_index]"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "dcbc8b189daf6dea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:56:39.736087Z",
     "start_time": "2024-04-27T22:56:39.723862Z"
    }
   },
   "source": [
    "vdat"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           vec1      vec2      vec3      vec4      vec5      vec6      vec7  \\\n",
       "0      0.212466 -0.042737  0.084530  0.234582 -0.099773 -0.061637  0.058663   \n",
       "1      0.329551 -0.031669 -0.051305 -0.084200  0.057401  0.038061  0.136885   \n",
       "2      0.416767 -0.063856  0.078748  0.057971 -0.059908 -0.073256 -0.056827   \n",
       "3      0.347848 -0.108581 -0.083247 -0.047303  0.275795 -0.014486  0.002262   \n",
       "4      0.254178 -0.043507  0.020654  0.090618 -0.097636 -0.013794 -0.006343   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "55494  0.161414 -0.036910 -0.071791 -0.005227  0.108638  0.007475 -0.125491   \n",
       "55495  0.582511 -0.159423 -0.021621 -0.058352  0.019498  0.098623 -0.051045   \n",
       "55496  0.417243 -0.134113 -0.088882 -0.136230 -0.024750  0.007203 -0.051164   \n",
       "55497  0.259505 -0.015983 -0.044274  0.131172 -0.068296 -0.085412  0.032562   \n",
       "55498  0.465567 -0.174543 -0.052301 -0.083729  0.072768  0.034405 -0.126774   \n",
       "\n",
       "           vec8      vec9     vec10  ...    vec291    vec292    vec293  \\\n",
       "0      0.002035 -0.061615 -0.012356  ...  0.023678 -0.022664  0.023196   \n",
       "1      0.167399  0.085140 -0.040155  ...  0.004887  0.001357 -0.047930   \n",
       "2      0.002724  0.164222  0.016892  ...  0.030218  0.054838  0.025151   \n",
       "3      0.001086 -0.116583  0.019576  ...  0.013997 -0.014818  0.004612   \n",
       "4      0.042713 -0.015163 -0.045401  ...  0.022082 -0.029517  0.014473   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "55494  0.069914 -0.135872 -0.025866  ...  0.000938  0.013113  0.002274   \n",
       "55495 -0.039033 -0.140285 -0.017645  ...  0.002389  0.013619  0.034793   \n",
       "55496 -0.052207 -0.020424  0.027685  ... -0.029437 -0.049406  0.023790   \n",
       "55497  0.038944  0.145036 -0.034650  ...  0.015484 -0.009464 -0.006393   \n",
       "55498 -0.016723 -0.112606  0.003462  ...  0.006384 -0.022939  0.024563   \n",
       "\n",
       "         vec294    vec295    vec296    vec297    vec298    vec299    vec300  \n",
       "0      0.015490  0.005477  0.009341  0.022774  0.017434  0.000098 -0.036202  \n",
       "1     -0.018181  0.019025 -0.018446 -0.015602 -0.002173 -0.015745  0.004060  \n",
       "2     -0.000195  0.010810  0.009157 -0.001221 -0.051728 -0.001580 -0.012022  \n",
       "3     -0.001743  0.017743  0.005413  0.008091  0.007267 -0.016822  0.027371  \n",
       "4     -0.003305 -0.000267  0.008892  0.001569  0.014592 -0.019717  0.002070  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "55494  0.010872 -0.003411 -0.016182 -0.027674  0.000669  0.039991  0.011887  \n",
       "55495  0.019187  0.006064 -0.009528  0.012671 -0.006700  0.002438 -0.005178  \n",
       "55496 -0.035472  0.031234 -0.008313  0.006281  0.019443 -0.000412  0.051274  \n",
       "55497  0.004798  0.011966  0.034469  0.001874  0.013438 -0.003467  0.014037  \n",
       "55498 -0.027063  0.022273  0.000543 -0.010527  0.028513 -0.021941 -0.010064  \n",
       "\n",
       "[55499 rows x 300 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vec1</th>\n",
       "      <th>vec2</th>\n",
       "      <th>vec3</th>\n",
       "      <th>vec4</th>\n",
       "      <th>vec5</th>\n",
       "      <th>vec6</th>\n",
       "      <th>vec7</th>\n",
       "      <th>vec8</th>\n",
       "      <th>vec9</th>\n",
       "      <th>vec10</th>\n",
       "      <th>...</th>\n",
       "      <th>vec291</th>\n",
       "      <th>vec292</th>\n",
       "      <th>vec293</th>\n",
       "      <th>vec294</th>\n",
       "      <th>vec295</th>\n",
       "      <th>vec296</th>\n",
       "      <th>vec297</th>\n",
       "      <th>vec298</th>\n",
       "      <th>vec299</th>\n",
       "      <th>vec300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.212466</td>\n",
       "      <td>-0.042737</td>\n",
       "      <td>0.084530</td>\n",
       "      <td>0.234582</td>\n",
       "      <td>-0.099773</td>\n",
       "      <td>-0.061637</td>\n",
       "      <td>0.058663</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>-0.061615</td>\n",
       "      <td>-0.012356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023678</td>\n",
       "      <td>-0.022664</td>\n",
       "      <td>0.023196</td>\n",
       "      <td>0.015490</td>\n",
       "      <td>0.005477</td>\n",
       "      <td>0.009341</td>\n",
       "      <td>0.022774</td>\n",
       "      <td>0.017434</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>-0.036202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.329551</td>\n",
       "      <td>-0.031669</td>\n",
       "      <td>-0.051305</td>\n",
       "      <td>-0.084200</td>\n",
       "      <td>0.057401</td>\n",
       "      <td>0.038061</td>\n",
       "      <td>0.136885</td>\n",
       "      <td>0.167399</td>\n",
       "      <td>0.085140</td>\n",
       "      <td>-0.040155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004887</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>-0.047930</td>\n",
       "      <td>-0.018181</td>\n",
       "      <td>0.019025</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.015602</td>\n",
       "      <td>-0.002173</td>\n",
       "      <td>-0.015745</td>\n",
       "      <td>0.004060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.416767</td>\n",
       "      <td>-0.063856</td>\n",
       "      <td>0.078748</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>-0.059908</td>\n",
       "      <td>-0.073256</td>\n",
       "      <td>-0.056827</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>0.164222</td>\n",
       "      <td>0.016892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030218</td>\n",
       "      <td>0.054838</td>\n",
       "      <td>0.025151</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>0.010810</td>\n",
       "      <td>0.009157</td>\n",
       "      <td>-0.001221</td>\n",
       "      <td>-0.051728</td>\n",
       "      <td>-0.001580</td>\n",
       "      <td>-0.012022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.347848</td>\n",
       "      <td>-0.108581</td>\n",
       "      <td>-0.083247</td>\n",
       "      <td>-0.047303</td>\n",
       "      <td>0.275795</td>\n",
       "      <td>-0.014486</td>\n",
       "      <td>0.002262</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>-0.116583</td>\n",
       "      <td>0.019576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013997</td>\n",
       "      <td>-0.014818</td>\n",
       "      <td>0.004612</td>\n",
       "      <td>-0.001743</td>\n",
       "      <td>0.017743</td>\n",
       "      <td>0.005413</td>\n",
       "      <td>0.008091</td>\n",
       "      <td>0.007267</td>\n",
       "      <td>-0.016822</td>\n",
       "      <td>0.027371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.254178</td>\n",
       "      <td>-0.043507</td>\n",
       "      <td>0.020654</td>\n",
       "      <td>0.090618</td>\n",
       "      <td>-0.097636</td>\n",
       "      <td>-0.013794</td>\n",
       "      <td>-0.006343</td>\n",
       "      <td>0.042713</td>\n",
       "      <td>-0.015163</td>\n",
       "      <td>-0.045401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022082</td>\n",
       "      <td>-0.029517</td>\n",
       "      <td>0.014473</td>\n",
       "      <td>-0.003305</td>\n",
       "      <td>-0.000267</td>\n",
       "      <td>0.008892</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>0.014592</td>\n",
       "      <td>-0.019717</td>\n",
       "      <td>0.002070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55494</th>\n",
       "      <td>0.161414</td>\n",
       "      <td>-0.036910</td>\n",
       "      <td>-0.071791</td>\n",
       "      <td>-0.005227</td>\n",
       "      <td>0.108638</td>\n",
       "      <td>0.007475</td>\n",
       "      <td>-0.125491</td>\n",
       "      <td>0.069914</td>\n",
       "      <td>-0.135872</td>\n",
       "      <td>-0.025866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.013113</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.010872</td>\n",
       "      <td>-0.003411</td>\n",
       "      <td>-0.016182</td>\n",
       "      <td>-0.027674</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.039991</td>\n",
       "      <td>0.011887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55495</th>\n",
       "      <td>0.582511</td>\n",
       "      <td>-0.159423</td>\n",
       "      <td>-0.021621</td>\n",
       "      <td>-0.058352</td>\n",
       "      <td>0.019498</td>\n",
       "      <td>0.098623</td>\n",
       "      <td>-0.051045</td>\n",
       "      <td>-0.039033</td>\n",
       "      <td>-0.140285</td>\n",
       "      <td>-0.017645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.013619</td>\n",
       "      <td>0.034793</td>\n",
       "      <td>0.019187</td>\n",
       "      <td>0.006064</td>\n",
       "      <td>-0.009528</td>\n",
       "      <td>0.012671</td>\n",
       "      <td>-0.006700</td>\n",
       "      <td>0.002438</td>\n",
       "      <td>-0.005178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55496</th>\n",
       "      <td>0.417243</td>\n",
       "      <td>-0.134113</td>\n",
       "      <td>-0.088882</td>\n",
       "      <td>-0.136230</td>\n",
       "      <td>-0.024750</td>\n",
       "      <td>0.007203</td>\n",
       "      <td>-0.051164</td>\n",
       "      <td>-0.052207</td>\n",
       "      <td>-0.020424</td>\n",
       "      <td>0.027685</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029437</td>\n",
       "      <td>-0.049406</td>\n",
       "      <td>0.023790</td>\n",
       "      <td>-0.035472</td>\n",
       "      <td>0.031234</td>\n",
       "      <td>-0.008313</td>\n",
       "      <td>0.006281</td>\n",
       "      <td>0.019443</td>\n",
       "      <td>-0.000412</td>\n",
       "      <td>0.051274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55497</th>\n",
       "      <td>0.259505</td>\n",
       "      <td>-0.015983</td>\n",
       "      <td>-0.044274</td>\n",
       "      <td>0.131172</td>\n",
       "      <td>-0.068296</td>\n",
       "      <td>-0.085412</td>\n",
       "      <td>0.032562</td>\n",
       "      <td>0.038944</td>\n",
       "      <td>0.145036</td>\n",
       "      <td>-0.034650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015484</td>\n",
       "      <td>-0.009464</td>\n",
       "      <td>-0.006393</td>\n",
       "      <td>0.004798</td>\n",
       "      <td>0.011966</td>\n",
       "      <td>0.034469</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>0.013438</td>\n",
       "      <td>-0.003467</td>\n",
       "      <td>0.014037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55498</th>\n",
       "      <td>0.465567</td>\n",
       "      <td>-0.174543</td>\n",
       "      <td>-0.052301</td>\n",
       "      <td>-0.083729</td>\n",
       "      <td>0.072768</td>\n",
       "      <td>0.034405</td>\n",
       "      <td>-0.126774</td>\n",
       "      <td>-0.016723</td>\n",
       "      <td>-0.112606</td>\n",
       "      <td>0.003462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006384</td>\n",
       "      <td>-0.022939</td>\n",
       "      <td>0.024563</td>\n",
       "      <td>-0.027063</td>\n",
       "      <td>0.022273</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>-0.010527</td>\n",
       "      <td>0.028513</td>\n",
       "      <td>-0.021941</td>\n",
       "      <td>-0.010064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55499 rows Ã— 300 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "1fa0ded1830e9a80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:56:39.757879Z",
     "start_time": "2024-04-27T22:56:39.736638Z"
    }
   },
   "source": [
    "# Check for NaN values in the 'Consumer complaint narrative' column\n",
    "nan_values = data['Consumer complaint narrative'].isnull()\n",
    "\n",
    "\n",
    "data.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 55499 entries, 18/02/2024 to 09/07/2021\n",
      "Data columns (total 17 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   Product                       55499 non-null  object \n",
      " 1   Sub-product                   55499 non-null  object \n",
      " 2   Issue                         55499 non-null  object \n",
      " 3   Sub-issue                     55499 non-null  object \n",
      " 4   Consumer complaint narrative  55499 non-null  object \n",
      " 5   Company public response       55499 non-null  object \n",
      " 6   Company                       55499 non-null  object \n",
      " 7   State                         55499 non-null  object \n",
      " 8   ZIP code                      55499 non-null  object \n",
      " 9   Tags                          55499 non-null  object \n",
      " 10  Consumer consent provided?    55499 non-null  object \n",
      " 11  Submitted via                 55499 non-null  object \n",
      " 12  Date sent to company          55499 non-null  object \n",
      " 13  Company response to consumer  55499 non-null  object \n",
      " 14  Timely response?              55499 non-null  object \n",
      " 15  Consumer disputed?            0 non-null      float64\n",
      " 16  Complaint ID                  55499 non-null  int64  \n",
      "dtypes: float64(1), int64(1), object(15)\n",
      "memory usage: 7.6+ MB\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "93e5a6f6dab9dd86",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "5133fcb8034f09d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:56:39.759641Z",
     "start_time": "2024-04-27T22:56:39.758566Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "9ef72d90ff586106",
   "metadata": {},
   "source": [
    "#############################################\n",
    "# Train a vector classifier\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "id": "857762f8d840bae5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T22:56:39.957977Z",
     "start_time": "2024-04-27T22:56:39.760011Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit the encoder and transform the 'Company response to consumer' column in the training data\n",
    "# This converts the categorical data into numerical data which can be used for machine learning\n",
    "data_train['Company response to consumer'] = le.fit_transform(data_train['Company response to consumer'])\n",
    "\n",
    "# Transform the 'Company response to consumer' column in the testing data using the fitted encoder\n",
    "# This ensures that the same categorical to numerical mapping is used for the testing data\n",
    "data_test['Company response to consumer'] = le.transform(data_test['Company response to consumer'])\n",
    "\n",
    "# Create a Lasso regression model with an alpha of 0.001\n",
    "# Lasso regression is a type of linear regression that uses shrinkage, where data values that are less absolute are shrunk towards zero\n",
    "Lasso_vec = Lasso(alpha = 0.001)\n",
    "\n",
    "# Fit the Lasso model using the training data\n",
    "# The 'Company response to consumer' column is the target variable, and the rest of the columns are the features\n",
    "Lasso_vec.fit(vdat_train,  data_train['Company response to consumer'])\n",
    "\n",
    "# Use the fitted Lasso model to make predictions on the testing data\n",
    "test_predict = Lasso_vec.predict(vdat_test)\n",
    "\n",
    "# Estimate the accuracy of the model using Kendall's tau-a correlation coefficient\n",
    "# This measures the ordinal association between the predicted and actual values\n",
    "vec_acc = kendall_acc(test_predict, data_test['Company response to consumer'])\n",
    "\n",
    "# Print the estimated accuracy of the model\n",
    "print(vec_acc)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     acc  lower  upper\n",
      "0  60.82  60.18  61.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2y/ktn1hrps3flfg5b_bszm_lk00000gn/T/ipykernel_10740/3982208439.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_train['Company response to consumer'] = le.fit_transform(data_train['Company response to consumer'])\n",
      "/var/folders/2y/ktn1hrps3flfg5b_bszm_lk00000gn/T/ipykernel_10740/3982208439.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_test['Company response to consumer'] = le.transform(data_test['Company response to consumer'])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "873f6d5aa9937ae5",
   "metadata": {},
   "source": [
    "#############################################\n",
    "# vector embeddings + ngrams\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "id": "b33640dd01e3baf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T23:01:33.109507Z",
     "start_time": "2024-04-27T22:59:58.093982Z"
    }
   },
   "source": [
    "# Apply the TAB_dfm function to the 'Consumer complaint narrative' column of the training data\n",
    "# This function applies CountVectorizer to the input text and returns a DataFrame and a matrix representation of the text\n",
    "data_dfm_train, _ = TAB_dfm(data_train['Consumer complaint narrative'])\n",
    "\n",
    "# Apply the TAB_dfm function to the 'Consumer complaint narrative' column of the testing data\n",
    "# The minimum proportion of documents a word must be present in for it to be kept is set to 0\n",
    "data_dfm_test, _ = TAB_dfm(data_test['Consumer complaint narrative'], min_prop = 0)\n",
    "\n",
    "# Create a list of the column names of the training and testing DataFrames\n",
    "d = [list(data_dfm_train), list(data_dfm_test)]\n",
    "\n",
    "# Find the intersection of the column names of the training and testing DataFrames\n",
    "# This ensures that both DataFrames have the same columns\n",
    "col_heads = list(set.intersection(*map(set,d)))\n",
    "\n",
    "# Update the training and testing DataFrames to only include the intersecting columns\n",
    "# The index of the DataFrames is reset and the original index is dropped\n",
    "data_dfm_train= data_dfm_train[col_heads].reset_index(drop = True)\n",
    "data_dfm_test = data_dfm_test[col_heads].reset_index(drop = True)\n",
    "\n",
    "# Reset the index of the training and testing DataFrames containing the transformed data\n",
    "vdat_train = vdat_train.reset_index(drop = True)\n",
    "vdat_test = vdat_test.reset_index(drop = True)\n",
    "\n",
    "# Concatenate the training DataFrames along the columns\n",
    "# This creates a new DataFrame that includes both the transformed data and the vectorized text data\n",
    "combined_x_train = pd.concat([vdat_train, data_dfm_train], axis = 1)\n",
    "\n",
    "# Concatenate the testing DataFrames along the columns\n",
    "# This creates a new DataFrame that includes both the transformed data and the vectorized text data\n",
    "combined_x_test = pd.concat([vdat_test, data_dfm_test], axis = 1)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "e46e12856d7e89d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T23:01:33.113427Z",
     "start_time": "2024-04-27T23:01:33.110957Z"
    }
   },
   "source": [
    "print(vdat_train.shape)\n",
    "print(data_dfm_train.shape)\n",
    "print(combined_x_train.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33299, 300)\n",
      "(33299, 1318)\n",
      "(33299, 1618)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "1f862465a9e16fcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T23:02:13.967403Z",
     "start_time": "2024-04-27T23:01:33.113988Z"
    }
   },
   "source": [
    "# Import the Lasso model from sklearn.linear_model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Instantiate a Lasso model with an alpha of 0.001\n",
    "# Lasso is a regression analysis method that performs both variable selection and regularization\n",
    "# Alpha is a constant that multiplies the L1 term in the Lasso regression equation\n",
    "# Smaller values of alpha result in less regularization and a model that fits the training data more closely\n",
    "lasso_all = Lasso(alpha = 0.001)\n",
    "\n",
    "# Fit the Lasso model to the training data\n",
    "# The target variable is 'Company response to consumer'\n",
    "# The features are all other columns in the training data\n",
    "lasso_all.fit(combined_x_train, data_train['Company response to consumer'])\n",
    "\n",
    "# Use the fitted Lasso model to make predictions on the testing data\n",
    "# The result is a list of predicted values for the target variable 'Company response to consumer'\n",
    "test_all_predict = lasso_all.predict(combined_x_test)\n",
    "\n",
    "# Estimate the accuracy of the model using Kendall's tau-a correlation coefficient\n",
    "\n",
    "ngram_vec_acc = kendall_acc(test_all_predict, data_test['Company response to consumer'])\n",
    "\n",
    "# Print the estimated accuracy of the model\n",
    "print(ngram_vec_acc)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     acc  lower  upper\n",
      "0  62.86  62.22  63.49\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "8171bdedab207491",
   "metadata": {},
   "source": [
    "#############################################\n",
    "# ngrams alone\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "id": "5acb954ceccf4134",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-27T23:11:37.130114Z"
    }
   },
   "source": [
    "# Instantiate a Lasso model with an alpha of 0.001 and maximum iteration of 10000\n",
    "# Smaller values of alpha result in less regularization and a model that fits the training data more closely\n",
    "# max_iter is the maximum number of iterations for the solver to converge\n",
    "lasso_dfm = Lasso(alpha = 0.001, max_iter=10000)\n",
    "\n",
    "# Fit the Lasso model to the training data\n",
    "# The target variable is 'Company response to consumer'\n",
    "\n",
    "lasso_all.fit(data_dfm_train, data_train['Company response to consumer'])\n",
    "\n",
    "# Use the fitted Lasso model to make predictions on the testing data\n",
    "# The result is a list of predicted values for the target variable 'Company response to consumer'\n",
    "test_dfm_predict = lasso_all.predict(data_dfm_test)\n",
    "\n",
    "# Estimate the accuracy of the model using Kendall's tau-a correlation coefficient\n",
    "\n",
    "ngram_acc = kendall_acc(test_dfm_predict, data_test['Company response to consumer'])\n",
    "\n",
    "# Print the estimated accuracy of the model\n",
    "print(ngram_acc)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f1c92379d64766f",
   "metadata": {},
   "source": [
    "########################################\n",
    "# Benchmarks\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "id": "1f6665580f6f3e6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T23:03:20.671706Z",
     "start_time": "2024-04-27T23:03:13.801035Z"
    }
   },
   "source": [
    "# Add a new column 'wdct' to the 'data_test' DataFrame\n",
    "# This column contains the word count of each 'Consumer complaint narrative'\n",
    "data_test['wdct'] = data_test['Consumer complaint narrative'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Add a new column 'sentiment' to the 'data_test' DataFrame\n",
    "# This column contains the sentiment polarity of each 'Consumer complaint narrative'\n",
    "# The sentiment polarity is calculated using the TextBlob library\n",
    "# The polarity is a float within the range [-1.0, 1.0] where -1 means negative sentiment and 1 means positive sentiment\n",
    "data_test['sentiment'] = data_test['Consumer complaint narrative'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "\n",
    "# Calculate the Kendall's tau-a correlation coefficient between the 'wdct' column and the 'Company response to consumer' column\n",
    "# This gives an estimate of the accuracy of using word count as a predictor for the company's response to the consumer\n",
    "wdct_acc = kendall_acc(data_test['wdct'], data_test['Company response to consumer'])\n",
    "\n",
    "# Calculate the Kendall's tau-a correlation coefficient between the 'sentiment' column and the 'Company response to consumer' column\n",
    "# This gives an estimate of the accuracy of using sentiment as a predictor for the company's response to the consumer\n",
    "sentiment_acc = kendall_acc(data_test['sentiment'], data_test['Company response to consumer'])\n",
    "\n",
    "# Print the estimated accuracy of using word count as a predictor\n",
    "print(wdct_acc)\n",
    "\n",
    "# Print the estimated accuracy of using sentiment as a predictor\n",
    "print(sentiment_acc)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2y/ktn1hrps3flfg5b_bszm_lk00000gn/T/ipykernel_10740/2849033226.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_test['wdct'] = data_test['Consumer complaint narrative'].apply(lambda x: len(str(x).split()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     acc  lower  upper\n",
      "0  49.34  48.68   50.0\n",
      "     acc  lower  upper\n",
      "0  52.98  52.32  53.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2y/ktn1hrps3flfg5b_bszm_lk00000gn/T/ipykernel_10740/2849033226.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_test['sentiment'] = data_test['Consumer complaint narrative'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "fb8c8aab4612c384",
   "metadata": {},
   "source": [
    "########################################\n",
    "# Combine accuracy estimates for a plot\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "id": "2d2bcad3bad53942",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T23:03:20.675327Z",
     "start_time": "2024-04-27T23:03:20.672363Z"
    }
   },
   "source": [
    "# Concatenate the accuracy estimates from different models into a single DataFrame\n",
    "# The models include: ngram model, word2vec model, combined ngram and word2vec model, word count model, and sentiment model\n",
    "plot_dat = pd.concat([ngram_acc, vec_acc, ngram_vec_acc, wdct_acc, sentiment_acc])\n",
    "\n",
    "# Add a new column 'features' to the DataFrame\n",
    "# This column contains the names of the feature sets used by each model\n",
    "plot_dat['features'] = ['ngrams', 'w2v', 'ngrams+w2v', 'word count', 'sentiment']\n",
    "\n",
    "# Add a new column 'err' to the DataFrame\n",
    "# This column contains the error of the accuracy estimate for each model\n",
    "# The error is calculated as the difference between the accuracy and the lower bound of the confidence interval\n",
    "plot_dat['err'] = plot_dat['acc'] - plot_dat['lower']\n",
    "\n",
    "# Print the DataFrame\n",
    "# This is used to check the accuracy estimates and their errors for each model\n",
    "print(plot_dat)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     acc  lower  upper    features   err\n",
      "0  62.54  61.90  63.18      ngrams  0.64\n",
      "0  60.82  60.18  61.47         w2v  0.64\n",
      "0  62.86  62.22  63.49  ngrams+w2v  0.64\n",
      "0  49.34  48.68  50.00  word count  0.66\n",
      "0  52.98  52.32  53.64   sentiment  0.66\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "1daad3d6d47e9a62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T23:03:20.733701Z",
     "start_time": "2024-04-27T23:03:20.675932Z"
    }
   },
   "source": [
    "# Import the pyplot module from matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create a new figure\n",
    "fig = plt.figure()\n",
    "\n",
    "# Create an error bar plot\n",
    "# The y-values are the names of the feature sets used by each model\n",
    "# The x-values are the accuracy estimates for each model\n",
    "# The x-errors are the errors of the accuracy estimates for each model\n",
    "# The format of the markers is 'o' (circle), the color of the markers is blue, the line width of the error bars is 0.6, the size of the markers is 8, and the length of the error bar caps is 10\n",
    "plt.errorbar(y = plot_dat['features'], x = plot_dat['acc'], xerr=plot_dat['err'], fmt=\"o\", color=\"b\", elinewidth=.6, markersize=8, capsize=10)\n",
    "\n",
    "# Disable the grid\n",
    "plt.grid(False)\n",
    "\n",
    "# Draw a vertical line at x=50\n",
    "# The color of the line is light grey and the line style is '-'\n",
    "plt.axvline(x=50, color='lightgrey', linestyle='-')\n",
    "\n",
    "# Set the labels of the x-axis and y-axis\n",
    "plt.xlabel('Accuracy', fontsize=18)\n",
    "plt.ylabel('Feature set', fontsize=18)\n",
    "\n",
    "# Set the margins of the plot\n",
    "# The x-margin is 0.1 and the y-margin is automatically adjusted to be tight\n",
    "plt.margins(0.1, tight=True)\n",
    "\n",
    "# Set the limits of the x-axis\n",
    "# The right limit is the maximum accuracy plus the maximum error, multiplied by 1.02\n",
    "# The left limit is the minimum accuracy minus the minimum error, multiplied by 0.98\n",
    "plt.xlim(right=((max(plot_dat['acc']) + max(plot_dat['err']))) * 1.02,\n",
    "         left=((min(plot_dat['acc']) - min(plot_dat['err']))) * 0.98)\n",
    "\n",
    "# Set the font size of the x-ticks and y-ticks to 14\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
