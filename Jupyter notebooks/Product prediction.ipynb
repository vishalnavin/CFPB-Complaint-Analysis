{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T03:28:30.644780Z",
     "start_time": "2024-04-29T03:28:30.633454Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Initialize SnowballStemmer with English language\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# Import necessary libraries for machine learning and text processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader as api\n",
    "import spacy\n",
    "\n",
    "def stemming_tokenizer(str_input):\n",
    "    \"\"\"\n",
    "    This function tokenizes the input string and applies stemming to each token.\n",
    "\n",
    "    Parameters:\n",
    "    str_input (str): The input string to be tokenized and stemmed.\n",
    "\n",
    "    Returns:\n",
    "    list: The list of stemmed tokens.\n",
    "    \"\"\"\n",
    "    words = re.sub(r\"[^A-Za-z]\", \" \", str_input).lower().split()\n",
    "    words = [snow_stemmer.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "def TAB_dfm(text, ngrams_range = (1,2), stop_words = 'english', min_prop = .01, max_features=None):\n",
    "    \"\"\"\n",
    "    This function applies CountVectorizer to the input text and returns a DataFrame and a matrix representation of the text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be vectorized.\n",
    "    ngrams_range (tuple): The range of n-values for different n-grams to be extracted.\n",
    "    stop_words (str): 'english' if English stop words are to be removed, else False.\n",
    "    min_prop (float): The minimum proportion of documents a word must be present in for it to be kept.\n",
    "    max_features (int): The maximum number of features to be kept, based on term frequency.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame representation of the text.\n",
    "    numpy.matrix: Matrix representation of the text.\n",
    "    \"\"\"\n",
    "    if stop_words == 'english':\n",
    "        vec = CountVectorizer(\n",
    "            tokenizer = stemming_tokenizer,\n",
    "            stop_words = stop_words,\n",
    "            ngram_range=ngrams_range,\n",
    "            min_df=min_prop,\n",
    "            max_features=max_features,\n",
    "            token_pattern='(?u)\\\\b\\\\w+\\\\b'\n",
    "            )\n",
    "    else:\n",
    "        vec = CountVectorizer(\n",
    "            tokenizer = stemming_tokenizer,\n",
    "            ngram_range=ngrams_range,\n",
    "            min_df=min_prop,\n",
    "            max_features=max_features,\n",
    "            token_pattern='(?u)\\\\b\\\\w+\\\\b'\n",
    "        )\n",
    "\n",
    "    mtx = vec.fit_transform(text).todense()\n",
    "    df = round(pd.DataFrame(mtx, columns=vec.get_feature_names_out()),2)\n",
    "    return df, mtx\n",
    "\n",
    "def kendall_acc(x,y,percentage = True):\n",
    "    \"\"\"\n",
    "    This function calculates the Kendall's tau-a correlation coefficient between two lists.\n",
    "\n",
    "    Parameters:\n",
    "    x, y (list): The two lists for which to calculate the correlation coefficient.\n",
    "    percentage (bool): If True, the result is returned as a percentage.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the correlation coefficient, lower and upper confidence intervals.\n",
    "    \"\"\"\n",
    "    tau, p_value = stats.kendalltau(x, y)\n",
    "    tau_acc = .5+tau/2\n",
    "    tau_se = np.sqrt((tau_acc*(1 - tau_acc))/len(x))\n",
    "    report = pd.DataFrame([tau_acc, tau_acc - 1.96 * tau_se, tau_acc + 1.96 * tau_se],\n",
    "                          index = ['acc', 'lower', 'upper']).T\n",
    "    report = round(report,4)\n",
    "\n",
    "    if percentage is True:\n",
    "        report = report * 100\n",
    "\n",
    "    return report\n",
    "\n",
    "def jaccard_sim(str1, str2):\n",
    "    \"\"\"\n",
    "    This function calculates the Jaccard similarity between two strings.\n",
    "\n",
    "    Parameters:\n",
    "    str1, str2 (str): The two strings for which to calculate the Jaccard similarity.\n",
    "\n",
    "    Returns:\n",
    "    float: The Jaccard similarity between the two strings.\n",
    "    \"\"\"\n",
    "    a = set(stemming_tokenizer(str1))\n",
    "    b = set(stemming_tokenizer(str2))\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "def euclidian_dist(docs, y = 0):\n",
    "    \"\"\"\n",
    "    This function calculates the Euclidean distance between the vectors of a list of documents and a specific document.\n",
    "\n",
    "    Parameters:\n",
    "    docs (list): The list of documents.\n",
    "    y (int): The index of the specific document.\n",
    "\n",
    "    Returns:\n",
    "    list: The list of Euclidean distances.\n",
    "    \"\"\"\n",
    "    _, features = np.asarray(TAB_dfm(docs))\n",
    "    distances = [round(float(euclidean_distances([features[y]], [f])),2) for f in features]\n",
    "    return distances\n",
    "\n",
    "def cosine_sim(docs, y = 0):\n",
    "    \"\"\"\n",
    "    This function calculates the cosine similarity between the vectors of a list of documents and a specific document.\n",
    "\n",
    "    Parameters:\n",
    "    docs (list): The list of documents.\n",
    "    y (int): The index of the specific document.\n",
    "\n",
    "    Returns:\n",
    "    list: The list of cosine similarities.\n",
    "    \"\"\"\n",
    "    _, features = np.asarray(TAB_dfm(docs, stop_words = False))\n",
    "    distances = [round(float(cosine_similarity([features[y]], [f])),2) for f in features]\n",
    "    return distances\n",
    "\n",
    "def spacy_parse(text):\n",
    "    \"\"\"\n",
    "    This function parses a text using the Spacy library and returns a DataFrame containing the parsed information.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to be parsed.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the parsed information.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    rows = [[t.text, t.lemma_, t.pos_, t.tag_, t.dep_, spacy.explain(t.pos_), t.is_stop] for t in doc]\n",
    "    cols = (\"text\", \"lemma\", \"POS\", \"Tag\",\"Dep\",\"explain\", \"stopword\")\n",
    "    df = pd.DataFrame(rows, columns=cols)\n",
    "    return df\n",
    "\n",
    "def lemmas_parse(text):\n",
    "    \"\"\"\n",
    "    This function parses a text and returns a string of lemmas that are not pronouns, numbers, symbols, stopwords, spaces or punctuations.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to be parsed.\n",
    "\n",
    "    Returns:\n",
    "    str: A string of lemmas.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    lemmas = [t.lemma_ for t in doc\n",
    "              if t.pos_ not in ('SPACE', 'PRON', 'PUNCT', 'NUM', 'SYM')\n",
    "              if t.is_stop == False]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "def lemmas_dfm(texts):\n",
    "    \"\"\"\n",
    "    This function applies the lemmas_parse function to a list of texts and returns a DataFrame of the results.\n",
    "\n",
    "    Parameters:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame of the results.\n",
    "    \"\"\"\n",
    "    dfms_joined = pd.DataFrame()\n",
    "    for text in texts:\n",
    "        text = [lemmas_parse(text)]\n",
    "        if len(text[0]) > 1:\n",
    "            dfm, _ = TAB_dfm(text, ngrams_range=(0,1), stop_words = False)\n",
    "            dfms_joined = dfms_joined.append(dfm)\n",
    "    return dfms_joined\n",
    "\n",
    "def ner_parse(text):\n",
    "    \"\"\"\n",
    "    This function parses a text using the Spacy library and returns a DataFrame containing the named entities.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to be parsed.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the named entities.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    rows = [[ent.text, ent.start_char, ent.end_char, ent.label_] for ent in doc.ents]\n",
    "    cols = (\"Text\", \"Start\", \"End\", \"Label\")\n",
    "    df = pd.DataFrame(rows, columns=cols)\n",
    "    return df\n",
    "\n",
    "def ner_filter_parse(text):\n",
    "    \"\"\"\n",
    "    This function parses a text using the Spacy library and returns a string of unique named entities that are geographical places.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text to be parsed.\n",
    "\n",
    "    Returns:\n",
    "    str: A string of unique named entities that are geographical places.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    ents = [ent.text for ent in doc.ents\n",
    "            if ent.label_ == 'GPE']\n",
    "    return ' '.join(list(set(ents)))\n",
    "\n",
    "def ner_dfm(texts):\n",
    "    \"\"\"\n",
    "    This function applies the ner_filter_parse function to a list of texts and returns a DataFrame of the results.\n",
    "\n",
    "    Parameters:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame of the results.\n",
    "    \"\"\"\n",
    "    dfms_joined = pd.DataFrame()\n",
    "    for text in texts:\n",
    "        text = [ner_filter_parse(text)]\n",
    "        if len(text[0]) > 1:\n",
    "            dfm, _ = TAB_dfm(text, ngrams_range=(0,1), stop_words = False)\n",
    "            dfms_joined = dfms_joined.append(dfm)\n",
    "    return dfms_joined\n",
    "\n",
    "def tokenizer(str_input):\n",
    "    \"\"\"\n",
    "    This function tokenizes the input string.\n",
    "\n",
    "    Parameters:\n",
    "    str_input (str): The input string to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "    list: The list of tokens.\n",
    "    \"\"\"\n",
    "    words = re.sub(r\"[^A-Za-z]\", \" \", str_input).lower().split()\n",
    "    return words\n",
    "\n",
    "def dfm_lookup(text, dict_as_list, ngrams_range = (1,1), min_prop = .01, max_features=None):\n",
    "    \"\"\"\n",
    "    This function applies CountVectorizer to the input text and returns a DataFrame and a matrix representation of the text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be vectorized.\n",
    "    dict_as_list (list): The list of words to be used as the dictionary.\n",
    "    ngrams_range (tuple): The range of n-values for different n-grams to be extracted.\n",
    "    min_prop (float): The minimum proportion of documents a word must be present in for it to be kept.\n",
    "    max_features (int): The maximum number of features to be kept, based on term frequency.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame representation of the text.\n",
    "    numpy.matrix: Matrix representation of the text.\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer(\n",
    "        tokenizer = tokenizer,\n",
    "        stop_words = 'english',\n",
    "        ngram_range=ngrams_range,\n",
    "        min_df=min_prop,\n",
    "        max_features=max_features,\n",
    "        token_pattern='(?u)\\\\b\\\\w+\\\\b'\n",
    "        )\n",
    "\n",
    "    mtx = vec.fit_transform(text).todense()\n",
    "    df = round(pd.DataFrame(mtx, columns=vec.get_feature_names_out()),2)\n",
    "    df = df[df.columns.intersection(dict_as_list)]\n",
    "    row_sums = df.sum(axis=1)\n",
    "    return row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d14699f3f360ee6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T03:28:33.358499Z",
     "start_time": "2024-04-29T03:28:32.267093Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the pandas library as pd\n",
    "import pandas as pd\n",
    "\n",
    "# Read the 'vecSmall.csv' file\n",
    "# The first column of the file is used as the index of the DataFrame\n",
    "# The resulting DataFrame is stored in the variable 'vecSmall'\n",
    "vecSmall = pd.read_csv('vecSmall.csv', index_col= 0)\n",
    "\n",
    "# Read the 'wfFile.csv' file\n",
    "# The first column of the file is used as the index of the DataFrame\n",
    "# The resulting DataFrame is stored in the variable 'wfFile'\n",
    "wfFile = pd.read_csv('wfFile.csv', index_col= 0)\n",
    "\n",
    "# Read the 'filtered_data_new.csv' file\n",
    "# The first column of the file is used as the index of the DataFrame\n",
    "# The 'low_memory' parameter is set to False to silence a warning about column types\n",
    "# The resulting DataFrame is stored in the variable 'data'\n",
    "data = pd.read_csv('filtered_dataset.csv', index_col= 0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bdcf067c2f5dc55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T03:28:21.817193Z",
     "start_time": "2024-04-29T03:28:21.808275Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Instantiate the ShuffleSplit class with 1 split, a test size of 40%, and a random state of 42 for reproducibility\n",
    "# This will be used to create a random split of the data into training and testing sets\n",
    "sss = ShuffleSplit(n_splits=1, test_size=0.4, random_state = 42)\n",
    "\n",
    "# Get the number of splitting iterations in the cross-validator\n",
    "# This is not necessary for the split but can be used to check the number of splits\n",
    "sss.get_n_splits(data)\n",
    "\n",
    "# Generate indices to split data into training and test set\n",
    "# next() is used to get the next item from the iterator\n",
    "train_index, test_index = next(sss.split(data))\n",
    "\n",
    "# Use the generated indices to create the training set\n",
    "# iloc is used for indexing via integers\n",
    "data_train = data.iloc[train_index]\n",
    "\n",
    "# Use the generated indices to create the test set\n",
    "data_test = data.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc38de48e15f3d24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T03:28:21.823177Z",
     "start_time": "2024-04-29T03:28:21.818230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the lowercase function from the pyodbc library\n",
    "from pyodbc import lowercase\n",
    "\n",
    "# The following code is a Python equivalent to the vecCheck function in the vectorFunctions.R script\n",
    "\n",
    "# Define a pipeline for projecting data into embedding space\n",
    "# The pipeline consists of two steps:\n",
    "# 1. TfidfVectorizer: This is used to convert the text data into a matrix of TF-IDF features.\n",
    "#    The vocabulary is set to the index of the wfFile DataFrame and the lowercase parameter is set to False to keep uppercase characters.\n",
    "# 2. TruncatedSVD: This is used for dimensionality reduction. It transforms the data to have the same number of dimensions as the pre-trained model.\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(vocabulary=wfFile.index, lowercase=False)),  \n",
    "    ('lsa', TruncatedSVD(n_components=vecSmall.shape[1])),  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dc049ca77fd59c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T03:28:27.035221Z",
     "start_time": "2024-04-29T03:28:21.823753Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the pipeline to the 'Consumer complaint narrative' column of the data DataFrame\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This step involves transforming the text data into a matrix of TF-IDF features and then reducing the dimensionality of the data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConsumer complaint narrative\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Transform the 'Consumer complaint narrative' column of the data DataFrame using the fitted pipeline\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# This step involves projecting the data into the embedding space\u001b[39;00m\n\u001b[1;32m      7\u001b[0m vdat \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mtransform(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConsumer complaint narrative\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    404\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/decomposition/_truncated_svd.py:204\u001b[0m, in \u001b[0;36mTruncatedSVD.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit model on training data X.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    Returns the transformer object.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# param validation is done in fit_transform\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/decomposition/_truncated_svd.py:241\u001b[0m, in \u001b[0;36mTruncatedSVD.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;241m>\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    238\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be <=\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m n_features(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m         )\n\u001b[0;32m--> 241\u001b[0m     U, Sigma, VT \u001b[38;5;241m=\u001b[39m randomized_svd(\n\u001b[1;32m    242\u001b[0m         X,\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components,\n\u001b[1;32m    244\u001b[0m         n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter,\n\u001b[1;32m    245\u001b[0m         n_oversamples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_oversamples,\n\u001b[1;32m    246\u001b[0m         power_iteration_normalizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpower_iteration_normalizer,\n\u001b[1;32m    247\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[1;32m    248\u001b[0m     )\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents_ \u001b[38;5;241m=\u001b[39m VT\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# As a result of the SVD approximation error on X ~ U @ Sigma @ V.T,\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# X @ V is not the same as U @ Sigma\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:446\u001b[0m, in \u001b[0;36mrandomized_svd\u001b[0;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state, svd_lapack_driver)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transpose:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;66;03m# this implementation is a bit faster with smaller shape[1]\u001b[39;00m\n\u001b[1;32m    444\u001b[0m     M \u001b[38;5;241m=\u001b[39m M\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m--> 446\u001b[0m Q \u001b[38;5;241m=\u001b[39m randomized_range_finder(\n\u001b[1;32m    447\u001b[0m     M,\n\u001b[1;32m    448\u001b[0m     size\u001b[38;5;241m=\u001b[39mn_random,\n\u001b[1;32m    449\u001b[0m     n_iter\u001b[38;5;241m=\u001b[39mn_iter,\n\u001b[1;32m    450\u001b[0m     power_iteration_normalizer\u001b[38;5;241m=\u001b[39mpower_iteration_normalizer,\n\u001b[1;32m    451\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mrandom_state,\n\u001b[1;32m    452\u001b[0m )\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# project M to the (k + p) dimensional space using the basis vectors\u001b[39;00m\n\u001b[1;32m    455\u001b[0m B \u001b[38;5;241m=\u001b[39m safe_sparse_dot(Q\u001b[38;5;241m.\u001b[39mT, M)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:275\u001b[0m, in \u001b[0;36mrandomized_range_finder\u001b[0;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m power_iteration_normalizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLU\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    274\u001b[0m     Q, _ \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39mlu(safe_sparse_dot(A, Q), permute_l\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 275\u001b[0m     Q, _ \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39mlu(safe_sparse_dot(A\u001b[38;5;241m.\u001b[39mT, Q), permute_l\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m power_iteration_normalizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQR\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    277\u001b[0m     Q, _ \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39mqr(safe_sparse_dot(A, Q), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meconomic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/extmath.py:189\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    187\u001b[0m         ret \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a, b)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 189\u001b[0m     ret \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m@\u001b[39m b\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    192\u001b[0m     sparse\u001b[38;5;241m.\u001b[39missparse(a)\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    196\u001b[0m ):\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/scipy/sparse/_base.py:624\u001b[0m, in \u001b[0;36m_spbase.__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScalar operands are not allowed, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    623\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 624\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mul_dispatch(other)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/scipy/sparse/_base.py:526\u001b[0m, in \u001b[0;36m_spbase._mul_dispatch\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mul_vector(other\u001b[38;5;241m.\u001b[39mravel())\u001b[38;5;241m.\u001b[39mreshape(M, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m other\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m N:\n\u001b[0;32m--> 526\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mul_multivector(other)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mul_scalar(other)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/scipy/sparse/_compressed.py:501\u001b[0m, in \u001b[0;36m_cs_matrix._mul_multivector\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# csr_matvecs or csc_matvecs\u001b[39;00m\n\u001b[1;32m    500\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_sparsetools, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_matvecs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 501\u001b[0m fn(M, N, n_vecs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindptr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata,\n\u001b[1;32m    502\u001b[0m    other\u001b[38;5;241m.\u001b[39mravel(), result\u001b[38;5;241m.\u001b[39mravel())\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit the pipeline to the 'Consumer complaint narrative' column of the data DataFrame\n",
    "# This step involves transforming the text data into a matrix of TF-IDF features and then reducing the dimensionality of the data\n",
    "pipeline.fit(data['Consumer complaint narrative'])\n",
    "\n",
    "# Transform the 'Consumer complaint narrative' column of the data DataFrame using the fitted pipeline\n",
    "# This step involves projecting the data into the embedding space\n",
    "vdat = pipeline.transform(data['Consumer complaint narrative'])\n",
    "\n",
    "# Convert the embedded data into a DataFrame\n",
    "# The column names are generated dynamically based on the number of dimensions in the embedded data\n",
    "# Each column represents a dimension in the embedding space\n",
    "vdat = pd.DataFrame(vdat, columns=[f'vec{i+1}' for i in range(vdat.shape[1])])\n",
    "\n",
    "# Print the first 10 rows of the DataFrame\n",
    "# This is used to check the transformed data\n",
    "print(vdat.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9806b0bc9160231",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T03:28:27.036425Z",
     "start_time": "2024-04-29T03:28:27.036384Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select the training data from the transformed DataFrame 'vdat' using the training indices\n",
    "vdat_train = vdat.iloc[train_index]\n",
    "\n",
    "# Select the testing data from the transformed DataFrame 'vdat' using the testing indices\n",
    "vdat_test = vdat.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc8b189daf6dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e5a6f6dab9dd86",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ef72d90ff586106",
   "metadata": {},
   "source": [
    "#############################################\n",
    "# Train a vector classifier\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857762f8d840bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the frequency of each category in 'Product'\n",
    "frequency = data_train['Product'].value_counts(normalize=True)\n",
    "\n",
    "# Map the frequencies to the training data\n",
    "data_train['Product'] = data_train['Product'].map(frequency)\n",
    "\n",
    "# Map the frequencies to the testing data\n",
    "# Note: categories in the testing data that are not found in the training data will be replaced with NaN\n",
    "data_test['Product'] = data_test['Product'].map(frequency)\n",
    "\n",
    "# Now you can fit the Lasso model and make predictions\n",
    "Lasso_vec = Lasso(alpha = 0.001)\n",
    "Lasso_vec.fit(vdat_train,  data_train['Product'])\n",
    "test_predict = Lasso_vec.predict(vdat_test)\n",
    "\n",
    "# Estimate accuracy\n",
    "vec_acc = kendall_acc(test_predict, data_test['Product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca406d2fd6aecb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873f6d5aa9937ae5",
   "metadata": {},
   "source": [
    "#############################################\n",
    "# vector embeddings + ngrams\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33640dd01e3baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the TAB_dfm function to the 'Consumer complaint narrative' column of the training and testing data\n",
    "# The function returns a DataFrame and a matrix representation of the text\n",
    "# Only the DataFrame is used, so the matrix is discarded by assigning it to _\n",
    "data_dfm_train, _ = TAB_dfm(data_train['Consumer complaint narrative'])\n",
    "data_dfm_test, _ = TAB_dfm(data_test['Consumer complaint narrative'], min_prop = 0)\n",
    "\n",
    "# Create a list of lists containing the column names of the training and testing DataFrames\n",
    "d = [list(data_dfm_train), list(data_dfm_test)]\n",
    "\n",
    "# Find the intersection of the column names of the training and testing DataFrames\n",
    "# This is done to ensure that both DataFrames have the same columns\n",
    "col_heads = list(set.intersection(*map(set,d)))\n",
    "\n",
    "# Reset the indices of the training and testing DataFrames\n",
    "# This is done to ensure that the indices are consistent after concatenating the DataFrames\n",
    "data_dfm_train= data_dfm_train[col_heads].reset_index(drop = True)\n",
    "data_dfm_test = data_dfm_test[col_heads].reset_index(drop = True)\n",
    "vdat_train = vdat_train.reset_index(drop = True)\n",
    "vdat_test = vdat_test.reset_index(drop = True)\n",
    "\n",
    "# Concatenate the training DataFrames along the columns\n",
    "# This results in a DataFrame that includes both the vector embeddings and the n-grams\n",
    "combined_x_train = pd.concat([vdat_train, data_dfm_train], axis = 1)\n",
    "\n",
    "# Concatenate the testing DataFrames along the columns\n",
    "# This results in a DataFrame that includes both the vector embeddings and the n-grams\n",
    "combined_x_test = pd.concat([vdat_test, data_dfm_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e12856d7e89d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vdat_train.shape)\n",
    "print(data_dfm_train.shape)\n",
    "print(combined_x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f862465a9e16fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Lasso model with an alpha of 0.001\n",
    "# The alpha parameter controls the degree of regularization (higher values mean more regularization and simpler models)\n",
    "lasso_all = Lasso(alpha = 0.001)\n",
    "\n",
    "# Fit the Lasso model to the training data\n",
    "# The independent variables are the combined vector embeddings and n-grams (combined_x_train)\n",
    "# The dependent variable is the 'Product' column of the training data\n",
    "lasso_all.fit(combined_x_train, data_train['Product'])\n",
    "\n",
    "# Use the fitted model to make predictions on the testing data\n",
    "# The independent variables are the combined vector embeddings and n-grams of the testing data (combined_x_test)\n",
    "# The predictions are stored in the variable 'test_all_predict'\n",
    "test_all_predict = lasso_all.predict(combined_x_test)\n",
    "\n",
    "# Estimate the accuracy of the predictions\n",
    "\n",
    "# The accuracy estimate is stored in the variable 'ngram_vec_acc'\n",
    "ngram_vec_acc = kendall_acc(test_all_predict, data_test['Product'])\n",
    "\n",
    "# Print the accuracy estimate\n",
    "# This is used to check the performance of the model\n",
    "print(ngram_vec_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8171bdedab207491",
   "metadata": {},
   "source": [
    "#############################################\n",
    "# ngrams alone\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acb954ceccf4134",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lasso_dfm = Lasso(alpha = 0.001)\n",
    "\n",
    "# Fit the Lasso model to the training data\n",
    "# The independent variables are the n-grams (data_dfm_train)\n",
    "# The dependent variable is the 'Product' column of the training data\n",
    "lasso_all.fit(data_dfm_train, data_train['Product'])\n",
    "\n",
    "# Use the fitted model to make predictions on the testing data\n",
    "# The independent variables are the n-grams of the testing data (data_dfm_test)\n",
    "# The predictions are stored in the variable 'test_dfm_predict'\n",
    "test_dfm_predict = lasso_all.predict(data_dfm_test)\n",
    "\n",
    "# Estimate the accuracy of the predictions\n",
    "# The accuracy estimate is stored in the variable 'ngram_acc'\n",
    "ngram_acc = kendall_acc(test_dfm_predict, data_test['Product'])\n",
    "\n",
    "\n",
    "# This is used to check the performance of the model\n",
    "print(ngram_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a20c7a8183a8ab",
   "metadata": {},
   "source": [
    "#############################################\n",
    "# Logistic Regression\n",
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7acfc81a6f298371",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T03:28:42.088444Z",
     "start_time": "2024-04-29T03:28:40.115004Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "# The stop_words parameter is set to 'english' to remove English stop words from the text\n",
    "# The max_features parameter is set to 1000 to only consider the top 1000 terms ordered by term frequency\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "\n",
    "# Fit the TF-IDF Vectorizer to the 'Consumer complaint narrative' column of the data DataFrame\n",
    "# This step involves learning the vocabulary of the text and calculating the inverse document frequencies\n",
    "# Then, transform the text into a matrix of TF-IDF features\n",
    "X_tfidf = tfidf.fit_transform(data['Consumer complaint narrative'])\n",
    "\n",
    "# Split the TF-IDF features and the 'Product' column of the data DataFrame into training and testing sets\n",
    "# The test_size parameter is set to 0.6, meaning that 60% of the data will be used for testing and 40% for training\n",
    "# The random_state parameter is set to 42 for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, data['Product'], test_size=0.6, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d11a695adc3dd410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T03:28:44.802964Z",
     "start_time": "2024-04-29T03:28:43.742510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize logistic regression model\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd08214f297cb2e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T03:28:45.995751Z",
     "start_time": "2024-04-29T03:28:45.955178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     acc  lower  upper\n",
      "0  88.19  87.85  88.54\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "# Estimate the accuracy of the predictions\n",
    "\n",
    "logistic_acc = kendall_acc(y_pred, y_test)\n",
    "\n",
    "# Print the accuracy estimate\n",
    "# This is used to check the performance of the model\n",
    "print(logistic_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c92379d64766f",
   "metadata": {},
   "source": [
    "########################################\n",
    "# Benchmarks\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6665580f6f3e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'wdct' to the testing data DataFrame\n",
    "# The values of this column are the word counts of the 'Consumer complaint narrative' column\n",
    "# The word count is calculated by splitting the narrative into words and counting the number of words\n",
    "data_test['wdct'] = data_test['Consumer complaint narrative'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Add a new column 'sentiment' to the testing data DataFrame\n",
    "# The values of this column are the sentiment scores of the 'Consumer complaint narrative' column\n",
    "# The sentiment score is calculated using the TextBlob library, which returns a polarity score between -1 (negative) and 1 (positive)\n",
    "data_test['sentiment'] = data_test['Consumer complaint narrative'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "\n",
    "\n",
    "# The result is stored in the variable 'wdct_acc'\n",
    "wdct_acc = kendall_acc(data_test['wdct'], data_test['Product'])\n",
    "\n",
    "\n",
    "# The result is stored in the variable 'sentiment_acc'\n",
    "sentiment_acc = kendall_acc(data_test['sentiment'], data_test['Product'])\n",
    "\n",
    "\n",
    "# These are used to check the performance of the word count and sentiment score as predictors of the product\n",
    "print(wdct_acc)\n",
    "print(sentiment_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c8aab4612c384",
   "metadata": {},
   "source": [
    "########################################\n",
    "# Combine accuracy estimates for a plot\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2bcad3bad53942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the accuracy estimates from different models into a single DataFrame\n",
    "# The models include: ngram model, vector model, combined ngram and vector model, word count model, and sentiment model\n",
    "plot_dat = pd.concat([ngram_acc, vec_acc, ngram_vec_acc, wdct_acc, sentiment_acc, logistic_acc])\n",
    "\n",
    "# Add a new column 'features' to the DataFrame\n",
    "# The values of this column are the names of the feature sets used in the models\n",
    "plot_dat['features'] = ['ngrams', 'w2v', 'ngrams+w2v', 'word count', 'sentiment', 'logistic']\n",
    "\n",
    "# Add a new column 'err' to the DataFrame\n",
    "# The values of this column are the differences between the accuracy estimates and the lower confidence intervals\n",
    "# This is used to calculate the error bars for the plot\n",
    "plot_dat['err'] = plot_dat['acc'] - plot_dat['lower']\n",
    "\n",
    "# Print the DataFrame\n",
    "# This is used to check the accuracy estimates and error bars\n",
    "print(plot_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daad3d6d47e9a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.errorbar(y = plot_dat['features'], x = plot_dat['acc'], xerr=plot_dat['err'], fmt=\"o\", color=\"b\", elinewidth=.9, markersize=8, capsize=10)\n",
    "\n",
    "plt.grid(False)\n",
    "plt.axvline(x=50, color='lightgrey', linestyle='-')\n",
    "# add axis labels\n",
    "plt.xlabel('Accuracy', fontsize=18)\n",
    "plt.ylabel('Feature set', fontsize=18)\n",
    "# Makes the margins a bit wider (useful when there's only 2 points)\n",
    "plt.margins(0.1, tight=True)\n",
    "\n",
    "# set the height of the yaxis to be proportional to the data\n",
    "plt.xlim(right=((max(plot_dat['acc']) + max(plot_dat['err']))) * 1.02,\n",
    "         left=((min(plot_dat['acc']) - min(plot_dat['err']))) * 0.98)\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8433fc8b5b80cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
